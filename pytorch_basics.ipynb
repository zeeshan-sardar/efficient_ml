{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh/8o40HB/O1ugjh891OVU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeeshan-sardar/efficient_ml/blob/main/pytorch_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vHaSb0QpI86I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets # torchvision contains vision related datasets, transformations and model architectures.\n",
        "#There are other types as well like TorchText, TorchAudio\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torchvision Datasets object takes two arguments, one is the transforms and second target_transforms for images and labels respectively."
      ],
      "metadata": {
        "id": "Sxq0t0xQKU6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZx3Xjb_Jt04",
        "outputId": "1eb78db4-f3e1-4c26-91b3-b2549d62fece"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 11477234.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 206134.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3828617.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 15646577.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
      ],
      "metadata": {
        "id": "G13FEhxpLC1t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset=training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "c8fV5uSqLbzY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in train_dataloader:\n",
        "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "  print(f\"Shape of y: {y.shape}, {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlCNe4_IMI5q",
        "outputId": "b25243e3-c191-40e5-edec-2c31fbe27152"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]), torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Neural Network is defined by inheriting the `nn.Module`. The network architecture is defined in `__init__` function and the datapassing through the network is defined in `forward` function."
      ],
      "metadata": {
        "id": "pjKPzXuMOhRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),nn.ReLU(),\n",
        "        nn.Linear(512, 512), nn.ReLU(),\n",
        "        nn.Linear(512, 10))\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_ReH3BbMjck",
        "outputId": "a2d8b602-96ba-4131-cd3e-e8aacecd35cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fJLpvSvGPZnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For model training we need to have a loss function and optimizer.  \n",
        "During training loop, model makes the prediction, claculates the loss and backpropagrate it to correct it for the next iteration.\n",
        "\n",
        "Crossentropy loss is common for classification tasks."
      ],
      "metadata": {
        "id": "e77hVImjUhDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "1_oGLWMQUgga"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss.backward: it calculates the gradients of loos wrt each parameter of the model. The backpropagation happens using computational graph indicating which parameter is contributing how in the loss.\n",
        "\n",
        "optimizer.step: updates the parameters based on the gradients during backpropagation.\n",
        "\n",
        "Optimizer tells the update rule for model parameters, in this case it is stochastic gradient descent. The for updation is `param -= learning_rate * grad`.\n",
        "\n",
        "optimizer.zero_grad: resets the gradients after every batch to prevent gradient accumulation and increase stability.\n",
        "\n",
        "Putting it Together:\n",
        "\n",
        "Here's the overall flow of a training iteration:\n",
        "\n",
        "1. Calculate the loss for the current batch.\n",
        "2. Backpropagate to compute gradients.\n",
        "3. Update the model's parameters using the optimizer.\n",
        "4. Reset gradients for the next batch."
      ],
      "metadata": {
        "id": "Z2xQAUcTgNJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Parameters**\n",
        "Examples:\n",
        "In a linear layer: Weights represent connections between input and output neurons, and biases determine the activation thresholds.\n",
        "In a CNN: Filters capture spatial features in images, and biases regulate the activations of convolutional layers.\n",
        "In an LSTM: Weights govern the flow of information within the memory cell and gates.\n",
        "\n",
        "you can check which parts of your model are considered parameters using the model.parameters()"
      ],
      "metadata": {
        "id": "tFD1VgwBiIkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
      ],
      "metadata": {
        "id": "L6VBNguhVqD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "QivCSa6pXGqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "BlbUtd4ZZTfS",
        "outputId": "ce4dfb58-e9ef-42c8-c2bc-ca6e715b4249",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302810  [   64/60000]\n",
            "loss: 2.298926  [ 6464/60000]\n",
            "loss: 2.278595  [12864/60000]\n",
            "loss: 2.271198  [19264/60000]\n",
            "loss: 2.266478  [25664/60000]\n",
            "loss: 2.240637  [32064/60000]\n",
            "loss: 2.238707  [38464/60000]\n",
            "loss: 2.217944  [44864/60000]\n",
            "loss: 2.197321  [51264/60000]\n",
            "loss: 2.173370  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 54.6%, Avg loss: 2.173082 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.177229  [   64/60000]\n",
            "loss: 2.178077  [ 6464/60000]\n",
            "loss: 2.127108  [12864/60000]\n",
            "loss: 2.137861  [19264/60000]\n",
            "loss: 2.096600  [25664/60000]\n",
            "loss: 2.040423  [32064/60000]\n",
            "loss: 2.061537  [38464/60000]\n",
            "loss: 2.000973  [44864/60000]\n",
            "loss: 1.987123  [51264/60000]\n",
            "loss: 1.914861  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.8%, Avg loss: 1.927784 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.955326  [   64/60000]\n",
            "loss: 1.936322  [ 6464/60000]\n",
            "loss: 1.830831  [12864/60000]\n",
            "loss: 1.860294  [19264/60000]\n",
            "loss: 1.753417  [25664/60000]\n",
            "loss: 1.698043  [32064/60000]\n",
            "loss: 1.715196  [38464/60000]\n",
            "loss: 1.628510  [44864/60000]\n",
            "loss: 1.634241  [51264/60000]\n",
            "loss: 1.521017  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.9%, Avg loss: 1.553971 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.616903  [   64/60000]\n",
            "loss: 1.587500  [ 6464/60000]\n",
            "loss: 1.443480  [12864/60000]\n",
            "loss: 1.505755  [19264/60000]\n",
            "loss: 1.382395  [25664/60000]\n",
            "loss: 1.371155  [32064/60000]\n",
            "loss: 1.386058  [38464/60000]\n",
            "loss: 1.318123  [44864/60000]\n",
            "loss: 1.340304  [51264/60000]\n",
            "loss: 1.233146  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.4%, Avg loss: 1.269534 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.346039  [   64/60000]\n",
            "loss: 1.333125  [ 6464/60000]\n",
            "loss: 1.169640  [12864/60000]\n",
            "loss: 1.268357  [19264/60000]\n",
            "loss: 1.138706  [25664/60000]\n",
            "loss: 1.161074  [32064/60000]\n",
            "loss: 1.184825  [38464/60000]\n",
            "loss: 1.128664  [44864/60000]\n",
            "loss: 1.154200  [51264/60000]\n",
            "loss: 1.068737  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 1.095613 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors\n",
        "Tensors are like numpy ndarrays but they are optimized for hardware acceleration and differentiation. Both np arrays and pytorch tensors can be converted from one to another using the bridge."
      ],
      "metadata": {
        "id": "Dcl-r75QvCJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wAnpxNziZVys"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1,2],[3,4]]\n",
        "np_data = np.array(data)\n",
        "tensor = torch.tensor(data)\n"
      ],
      "metadata": {
        "id": "6taI3XZ1vrN9",
        "outputId": "a544be23-9754-44d9-bc2c-3c5a495c1665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np_tensor = torch.from_numpy(np_data)\n",
        "np_tensor"
      ],
      "metadata": {
        "id": "LXfP1YRq36FQ",
        "outputId": "1609115f-5f0c-4324-b3a5-cc178892732c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np_tensor.numpy()"
      ],
      "metadata": {
        "id": "yyMryQUO39hB",
        "outputId": "000ed07f-d3a8-44f3-8d02-1a3a652eeac5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_like = torch.ones_like(np_tensor)\n",
        "data_like"
      ],
      "metadata": {
        "id": "W87k0GZ_zi5X",
        "outputId": "980df16c-6033-49b5-bf11-d7802859af9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1],\n",
              "        [1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand_like(np_tensor, dtype=torch.float16)"
      ],
      "metadata": {
        "id": "SwGPqMT_0NIG",
        "outputId": "8d1b6375-5e58-4067-f3fb-5f492375443b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2739, 0.5054],\n",
              "        [0.6294, 0.9458]], dtype=torch.float16)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand((2,3))"
      ],
      "metadata": {
        "id": "e4jYvtTk0ZRu",
        "outputId": "2b4e40af-67b8-40e0-98f0-8b996b54b6e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3782, 0.0664, 0.0159],\n",
              "        [0.3111, 0.5670, 0.6933]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(2,3)"
      ],
      "metadata": {
        "id": "H5JGWtT81H0o"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default tensors are created in CPU and we have to move them explicitly to GPU to get the hardware acceleration. Keep in mind, copying large tensors can be time and memeory expensive."
      ],
      "metadata": {
        "id": "V0F2Ed271cJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to(\"cuda\")"
      ],
      "metadata": {
        "id": "WHdkj8qf1tdD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor.get_device()"
      ],
      "metadata": {
        "id": "2UxuQ_Hj2FGj",
        "outputId": "706194ef-9e0f-443c-bf82-bf8aadd50353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor can be converted into python numerical types using .item()\n",
        "\n",
        "tensor.sum().item()"
      ],
      "metadata": {
        "id": "BhtJOk6D2FwF",
        "outputId": "466dca5f-2bbf-44b7-ec60-273f8740ee26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.4518532752990723"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inplace operations can be acieved by *_ subscript.  Inplace operations can save memory but variables can lose their history immidiately and can be problamatic when computing derivatives. So, their usage is discouraged."
      ],
      "metadata": {
        "id": "mXcPnkN83D6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor.add_(3)"
      ],
      "metadata": {
        "id": "jkdBUAPX22J9",
        "outputId": "527b95ff-1cb5-4840-e3c0-2089c524eee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.7487, 6.1146, 6.7688],\n",
              "        [6.3831, 6.4955, 6.9412]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor"
      ],
      "metadata": {
        "id": "0ljS1IMl3I8O",
        "outputId": "e4b09efc-8cf3-4646-c618-1ee3d0c6fd36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.7487, 6.1146, 6.7688],\n",
              "        [6.3831, 6.4955, 6.9412]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8XSByt13ZHy"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}