{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwZeXEnXNcmxWLi8av3GN7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeeshan-sardar/efficient_ml/blob/main/pytorch_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vHaSb0QpI86I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets # torchvision contains vision related datasets, transformations and model architectures.\n",
        "#There are other types as well like TorchText, TorchAudio\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torchvision Datasets object takes two arguments, one is the transforms and second target_transforms for images and labels respectively."
      ],
      "metadata": {
        "id": "Sxq0t0xQKU6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZx3Xjb_Jt04",
        "outputId": "1eb78db4-f3e1-4c26-91b3-b2549d62fece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 11477234.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 206134.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3828617.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 15646577.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
      ],
      "metadata": {
        "id": "G13FEhxpLC1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset=training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "c8fV5uSqLbzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X, y in train_dataloader:\n",
        "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "  print(f\"Shape of y: {y.shape}, {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlCNe4_IMI5q",
        "outputId": "b25243e3-c191-40e5-edec-2c31fbe27152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]), torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Neural Network is defined by inheriting the `nn.Module`. The network architecture is defined in `__init__` function and the datapassing through the network is defined in `forward` function."
      ],
      "metadata": {
        "id": "pjKPzXuMOhRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),nn.ReLU(),\n",
        "        nn.Linear(512, 512), nn.ReLU(),\n",
        "        nn.Linear(512, 10))\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_ReH3BbMjck",
        "outputId": "a2d8b602-96ba-4131-cd3e-e8aacecd35cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fJLpvSvGPZnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For model training we need to have a loss function and optimizer.  \n",
        "During training loop, model makes the prediction, claculates the loss and backpropagrate it to correct it for the next iteration.\n",
        "\n",
        "Crossentropy loss is common for classification tasks."
      ],
      "metadata": {
        "id": "e77hVImjUhDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "1_oGLWMQUgga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss.backward: it calculates the gradients of loos wrt each parameter of the model. The backpropagation happens using computational graph indicating which parameter is contributing how in the loss.\n",
        "\n",
        "optimizer.step: updates the parameters based on the gradients during backpropagation.\n",
        "\n",
        "Optimizer tells the update rule for model parameters, in this case it is stochastic gradient descent. The for updation is `param -= learning_rate * grad`.\n",
        "\n",
        "optimizer.zero_grad: resets the gradients after every batch to prevent gradient accumulation and increase stability.\n",
        "\n",
        "Putting it Together:\n",
        "\n",
        "Here's the overall flow of a training iteration:\n",
        "\n",
        "1. Calculate the loss for the current batch.\n",
        "2. Backpropagate to compute gradients.\n",
        "3. Update the model's parameters using the optimizer.\n",
        "4. Reset gradients for the next batch."
      ],
      "metadata": {
        "id": "Z2xQAUcTgNJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Parameters**\n",
        "Examples:\n",
        "In a linear layer: Weights represent connections between input and output neurons, and biases determine the activation thresholds.\n",
        "In a CNN: Filters capture spatial features in images, and biases regulate the activations of convolutional layers.\n",
        "In an LSTM: Weights govern the flow of information within the memory cell and gates.\n",
        "\n",
        "you can check which parts of your model are considered parameters using the model.parameters()"
      ],
      "metadata": {
        "id": "tFD1VgwBiIkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
      ],
      "metadata": {
        "id": "L6VBNguhVqD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "QivCSa6pXGqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "BlbUtd4ZZTfS",
        "outputId": "ce4dfb58-e9ef-42c8-c2bc-ca6e715b4249",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302810  [   64/60000]\n",
            "loss: 2.298926  [ 6464/60000]\n",
            "loss: 2.278595  [12864/60000]\n",
            "loss: 2.271198  [19264/60000]\n",
            "loss: 2.266478  [25664/60000]\n",
            "loss: 2.240637  [32064/60000]\n",
            "loss: 2.238707  [38464/60000]\n",
            "loss: 2.217944  [44864/60000]\n",
            "loss: 2.197321  [51264/60000]\n",
            "loss: 2.173370  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 54.6%, Avg loss: 2.173082 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.177229  [   64/60000]\n",
            "loss: 2.178077  [ 6464/60000]\n",
            "loss: 2.127108  [12864/60000]\n",
            "loss: 2.137861  [19264/60000]\n",
            "loss: 2.096600  [25664/60000]\n",
            "loss: 2.040423  [32064/60000]\n",
            "loss: 2.061537  [38464/60000]\n",
            "loss: 2.000973  [44864/60000]\n",
            "loss: 1.987123  [51264/60000]\n",
            "loss: 1.914861  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.8%, Avg loss: 1.927784 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.955326  [   64/60000]\n",
            "loss: 1.936322  [ 6464/60000]\n",
            "loss: 1.830831  [12864/60000]\n",
            "loss: 1.860294  [19264/60000]\n",
            "loss: 1.753417  [25664/60000]\n",
            "loss: 1.698043  [32064/60000]\n",
            "loss: 1.715196  [38464/60000]\n",
            "loss: 1.628510  [44864/60000]\n",
            "loss: 1.634241  [51264/60000]\n",
            "loss: 1.521017  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.9%, Avg loss: 1.553971 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.616903  [   64/60000]\n",
            "loss: 1.587500  [ 6464/60000]\n",
            "loss: 1.443480  [12864/60000]\n",
            "loss: 1.505755  [19264/60000]\n",
            "loss: 1.382395  [25664/60000]\n",
            "loss: 1.371155  [32064/60000]\n",
            "loss: 1.386058  [38464/60000]\n",
            "loss: 1.318123  [44864/60000]\n",
            "loss: 1.340304  [51264/60000]\n",
            "loss: 1.233146  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.4%, Avg loss: 1.269534 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.346039  [   64/60000]\n",
            "loss: 1.333125  [ 6464/60000]\n",
            "loss: 1.169640  [12864/60000]\n",
            "loss: 1.268357  [19264/60000]\n",
            "loss: 1.138706  [25664/60000]\n",
            "loss: 1.161074  [32064/60000]\n",
            "loss: 1.184825  [38464/60000]\n",
            "loss: 1.128664  [44864/60000]\n",
            "loss: 1.154200  [51264/60000]\n",
            "loss: 1.068737  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 1.095613 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors\n",
        "Tensors are like numpy ndarrays but they are optimized for hardware acceleration and differentiation. Both np arrays and pytorch tensors can be converted from one to another using the bridge."
      ],
      "metadata": {
        "id": "Dcl-r75QvCJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wAnpxNziZVys"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1,2],[3,4]]\n",
        "np_data = np.array(data)\n",
        "tensor = torch.tensor(data)\n"
      ],
      "metadata": {
        "id": "6taI3XZ1vrN9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_tensor = torch.from_numpy(np_data)\n",
        "np_tensor"
      ],
      "metadata": {
        "id": "LXfP1YRq36FQ",
        "outputId": "1609115f-5f0c-4324-b3a5-cc178892732c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np_tensor.numpy()"
      ],
      "metadata": {
        "id": "yyMryQUO39hB",
        "outputId": "000ed07f-d3a8-44f3-8d02-1a3a652eeac5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_like = torch.ones_like(np_tensor)\n",
        "data_like"
      ],
      "metadata": {
        "id": "W87k0GZ_zi5X",
        "outputId": "980df16c-6033-49b5-bf11-d7802859af9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1],\n",
              "        [1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand_like(np_tensor, dtype=torch.float16)"
      ],
      "metadata": {
        "id": "SwGPqMT_0NIG",
        "outputId": "8d1b6375-5e58-4067-f3fb-5f492375443b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2739, 0.5054],\n",
              "        [0.6294, 0.9458]], dtype=torch.float16)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand((2,3))"
      ],
      "metadata": {
        "id": "e4jYvtTk0ZRu",
        "outputId": "2b4e40af-67b8-40e0-98f0-8b996b54b6e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3782, 0.0664, 0.0159],\n",
              "        [0.3111, 0.5670, 0.6933]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(2,3)"
      ],
      "metadata": {
        "id": "H5JGWtT81H0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default tensors are created in CPU and we have to move them explicitly to GPU to get the hardware acceleration. Keep in mind, copying large tensors can be time and memeory expensive."
      ],
      "metadata": {
        "id": "V0F2Ed271cJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to(\"cuda\")"
      ],
      "metadata": {
        "id": "WHdkj8qf1tdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor.get_device()"
      ],
      "metadata": {
        "id": "2UxuQ_Hj2FGj",
        "outputId": "706194ef-9e0f-443c-bf82-bf8aadd50353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor can be converted into python numerical types using .item()\n",
        "\n",
        "tensor.sum().item()"
      ],
      "metadata": {
        "id": "BhtJOk6D2FwF",
        "outputId": "466dca5f-2bbf-44b7-ec60-273f8740ee26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.4518532752990723"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inplace operations can be acieved by *_ subscript.  Inplace operations can save memory but variables can lose their history immidiately and can be problamatic when computing derivatives. So, their usage is discouraged."
      ],
      "metadata": {
        "id": "mXcPnkN83D6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor.add_(3)"
      ],
      "metadata": {
        "id": "jkdBUAPX22J9",
        "outputId": "527b95ff-1cb5-4840-e3c0-2089c524eee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.7487, 6.1146, 6.7688],\n",
              "        [6.3831, 6.4955, 6.9412]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor"
      ],
      "metadata": {
        "id": "0ljS1IMl3I8O",
        "outputId": "e4b09efc-8cf3-4646-c618-1ee3d0c6fd36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.7487, 6.1146, 6.7688],\n",
              "        [6.3831, 6.4955, 6.9412]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets and DataLoaders:\n",
        "Datasets help in creating an easy to use data handling object and saves us time and code space to to handle a bunch of data.\n",
        "\n",
        "DataLoaders gives an iteratble over Datasets object in batches and with shuffle options to shuffle the data after iterating over all batches.\n",
        "\n"
      ],
      "metadata": {
        "id": "nm5VkE9qEAZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a Datasets object for a custom dataset, we need to override three functions `__init()__`, `__len()__`, and `__getitem()__`.\n",
        "`__init()__` is called onces when the class gets instentiated. It takes the four arguments,\n",
        "1.   data input directory\n",
        "2.   data labels directory and filename\n",
        "3. transform of inputs\n",
        "4. transform for labels\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "juGBIFUCEjPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To load an image dataset in a custom fashion\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, img_dir, lable_file, transform = None, lable_transform = None):\n",
        "    self.img_dir = img_dir\n",
        "    self.lables = pd.read_csv(lable_file) # it contains two cols, one for image names and second for lable\n",
        "    self.transform = transform\n",
        "    self.lable_transform = lable_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lables)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.lables.iloc[idx, 0])\n",
        "    img = read_image(img_path)\n",
        "    lable = self.lables[idx, 1]\n",
        "\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    if self.lable_transform:\n",
        "      lable = self.lable_transform(lable)\n",
        "\n",
        "\n",
        "    return img, lable\n",
        "\n"
      ],
      "metadata": {
        "id": "q8XSByt13ZHy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforms\n",
        "Datasets, apart from img_dir and lable_dir, also takes two additional arguments which are transforms for both images and lables as callback functions.\n",
        "These transforms are required for training the DL model. The basic transforms for images is T0Tensor and for lables is one hot incoding for classification tasks.\n",
        "\n",
        "These transforms can be defined using lambda functions if not already defined.\n",
        "\n",
        "`ToTensor` converts PIL image or np ndarray to tensor floats and normalizes the range in [1,0].\n",
        "\n",
        "One hot encoding is generally preferred in classification tasks even if we have numerical lables. This helps in better gradient handling and softmax layer (output layers in most of the NN) also expects binanry on its input."
      ],
      "metadata": {
        "id": "KUSguGkeEi4-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCCe4aCIW5Hn",
        "outputId": "f50b99a2-bbaf-4688-ade1-4c4f3a2e5d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'Tensor' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b56815e10ed3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m tensor([[ 1,  2,  3,  4,  5],\n\u001b[0m\u001b[1;32m      4\u001b[0m         [ 6,  7,  8,  9, 10]])\n\u001b[1;32m      5\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pd)"
      ],
      "metadata": {
        "id": "BtvBsskSXKQa",
        "outputId": "c5b7d831-a7a3-4b34-e216-71d2b0012a64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zsK055IoXdqq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}