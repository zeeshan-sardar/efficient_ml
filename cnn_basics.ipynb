{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeeshan-sardar/efficient_ml/blob/main/cnn_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNNs are specifically designed for image related tasks. They focus on local features instead of looking the image as a whole.\n",
        "CNNs also help in reducing the numbre of parameters as compared to fully connected layers.\n",
        "\n",
        "## Important Aspects of CNNs\n",
        "\n",
        "### Kernels/Filters as Weights\n",
        "These kernels are small matrices (usually 1x1, 3x3 or 5x5) whcih are learned during network training to indentify specific patterns in images. These filters learn using backpropagation and gradient descent.\n",
        "\n",
        "In the early layers of the network, these filters learn simple patterns like edges and as we go deeper into the network they can detect more complex features like shapes. A particular kernel gets activated when it finds a similar pattern in the input image. These filters when applied on input image, produce feature maps.\n",
        "\n",
        "### PyTorch Tensor Format for CNNs\n",
        "PyTorch has a special format for image related data\n",
        "- Input Tensor (batch_size, in_channels, img_height, img_width)\n",
        "- Conv2D layer parameters (in_ch, out_ch, kernel_size, stride, padding)"
      ],
      "metadata": {
        "id": "ipaOcLAY5vGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "4y9SCz2xCt_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Input tensor\n",
        "input_tensor = torch.randn(1, 3, 8, 8)\n",
        "\n",
        "# Convolutional layer\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "# Apply convolution\n",
        "output_tensor = conv_layer(input_tensor)\n",
        "\n",
        "print(\"Input tensor size:\", input_tensor.size())\n",
        "print(\"Output tensor size:\", output_tensor.size())\n"
      ],
      "metadata": {
        "id": "-S-X-rEYKVqy",
        "outputId": "97206495-1306-422b-fd02-1e3ada59bd71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor size: torch.Size([1, 3, 8, 8])\n",
            "Output tensor size: torch.Size([1, 16, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the size of the weights\n",
        "print(\"Shape of conv_layer weights:\", conv_layer.weight.shape)\n",
        "\n",
        "# Print the weights of the first kernel\n",
        "print(\"Weights of the first kernel: \\n\", conv_layer.weight[0])"
      ],
      "metadata": {
        "id": "f99bSeLzOdzf",
        "outputId": "d3cece89-4b23-44bb-9d54-bb4029da93c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of conv_layer weights: torch.Size([16, 3, 3, 3])\n",
            "Weights of the first kernel: \n",
            " tensor([[[-0.1691, -0.0810, -0.1446],\n",
            "         [ 0.0615,  0.0248, -0.0155],\n",
            "         [-0.0579, -0.0669, -0.0626]],\n",
            "\n",
            "        [[ 0.1800,  0.0315, -0.1642],\n",
            "         [-0.1091,  0.1196, -0.1397],\n",
            "         [-0.0068, -0.0308, -0.0815]],\n",
            "\n",
            "        [[-0.0874, -0.1874,  0.0457],\n",
            "         [-0.0329,  0.1774,  0.0874],\n",
            "         [ 0.1300,  0.0244, -0.1758]]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the above example, the conv_layer.weight contains the information about kernels/filters which are the learnable parameters. There are total 16 filters each having shape of (3x3x3) because we have 3 input channels and we have defined 3x3 a kernel size. They are 16 because the output channels is 16 i this case.\n",
        "\n",
        "If we see the output tensor shape (1x16x8x8). These are the feature maps. Each filter 3x3x3 in this case, produces one feature map 8x8 in this case. We have 16 feature maps as of our output channels. The size of feature maps depends upon, input size, stride, padding and kernel size.\n",
        "\n",
        "## Parameter Update using Gradient Descent\n",
        "\n",
        "In machine learning and optimization, gradient descent is a common algorithm used to minimize a loss function. The loss function measures the difference between the predicted output of a model and the actual target values. The goal of training a model is to find the set of parameters (weights and biases) that minimize this loss function.\n",
        "\n",
        "### Gradient:\n",
        "\n",
        "The gradient of a function represents the rate of change of the function with respect to its parameters. In the context of neural networks, the gradient of the loss function with respect to the model parameters indicates how much the loss would change if the parameters were adjusted slightly.\n",
        "\n",
        "### Minimizing the Loss Function:\n",
        "\n",
        "To minimize the loss function, we want to adjust the model parameters in a way that reduces the loss. This adjustment is done by moving the parameters in the direction opposite to the gradient. This is because the gradient points in the direction of the steepest ascent, and moving in the opposite direction will lead to a decrease in the loss.\n",
        "\n",
        "### Opposite Direction of the Gradient:\n",
        "\n",
        "When we say \"the weights are adjusted in the opposite direction of the gradient,\" it means that we update the weights by subtracting a fraction of the gradient from the current weights. This fraction is determined by the learning rate, which controls the size of the steps taken during optimization.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Suppose we have a simple loss function \\(L(w)\\) that depends on a single weight parameter \\(w\\). We compute the gradient of the loss function with respect to \\(w\\) as \\(\\frac{dL}{dw}\\). If the gradient is positive, it means that increasing \\(w\\) will increase the loss, and decreasing \\(w\\) will decrease the loss. Therefore, to minimize the loss, we update \\(w\\) by subtracting a fraction of the gradient:\n",
        "\\[ w_{\\text{new}} = w_{\\text{old}} - \\text{learning\\_rate} \\times \\frac{dL}{dw} \\]\n",
        "If the gradient is negative, it means that decreasing \\(w\\) will increase the loss, and increasing \\(w\\) will decrease the loss. Therefore, we update \\(w\\) by adding a fraction of the gradient.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "Adjusting the weights in the opposite direction of the gradient is a fundamental principle in optimization algorithms like gradient descent. By iteratively updating the weights based on the gradient, the model gradually learns to minimize the loss function and improve its performance on the given task."
      ],
      "metadata": {
        "id": "vUZLNuSMCqLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q d2l"
      ],
      "metadata": {
        "id": "KVt7zkpQT3xZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}